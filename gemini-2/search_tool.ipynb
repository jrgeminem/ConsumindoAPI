{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrgeminem/ConsumindoAPI/blob/main/gemini-2/search_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lgX9omPXF-"
      },
      "source": [
        "## Gemini 2.0 - Search as a tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkR4fWudrHCs"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/gemini-2/search_tool.ipynb\"><img src=\"https://ai.google.dev/site-assets/images/docs/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKKNfXWrHgs"
      },
      "source": [
        "In this notebook you will learn how to use the new Google Search tool available in [Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2), using both the unary API and the Multimodal Live API. Check out the docs to learn more about using [Search as a tool](https://ai.google.dev/gemini-api/docs/models/gemini-v2#search-tool)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKu1tRBrQ7xj"
      },
      "source": [
        "## Set up the SDK\n",
        "\n",
        "This guide uses the [`google-genai`](https://pypi.org/project/google-genai) Python SDK to connect to the Gemini 2.0 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIWKUlPqP5NK"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://github.com/googleapis/python-genai)** provides programmatic access to Gemini 2 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs/models/gemini-v2) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://googleapis.github.io/python-genai/) or in the [Getting started](get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Fr84vJuPSHb"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a503bnWNQoCL"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RjvgYmdLQd5s",
        "collapsed": true,
        "outputId": "34add9ed-8417-4174-d9cf-fd5108b76689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fcf80f0bfd37>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgQ\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[AIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB-gQ] = userdata.get(AIzaSyBKQ1HZ9WT9RLzPzqX1PqNywh8IJ1YB-gQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKXgMSNQrrV"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "The client will pick up your API key from the environment variable.\n",
        "To use the live API you need to set the client version to `v1alpha` and use the Gemini 2.0 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C75s1LR9QmOz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(http_options={'api_version': 'v1alpha'})\n",
        "\n",
        "MODEL = 'gemini-2.0-flash-exp'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYGib8EySfBE"
      },
      "source": [
        "## Use search in chat\n",
        "\n",
        "Start by defining a helper function that you will use to display each part of the returned response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUTX1SsKS3CE"
      },
      "outputs": [],
      "source": [
        "# @title Define some helpers (run this cell)\n",
        "import json\n",
        "\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "\n",
        "def show_json(obj):\n",
        "  print(json.dumps(obj.model_dump(exclude_none=True), indent=2))\n",
        "\n",
        "def show_parts(r):\n",
        "  parts = r.candidates[0].content.parts\n",
        "  if parts is None:\n",
        "    finish_reason = r.candidates[0].finish_reason\n",
        "    print(f'{finish_reason=}')\n",
        "    return\n",
        "  for part in r.candidates[0].content.parts:\n",
        "    if part.text:\n",
        "      display(Markdown(part.text))\n",
        "    elif part.executable_code:\n",
        "      display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
        "    else:\n",
        "      show_json(part)\n",
        "\n",
        "  grounding_metadata = r.candidates[0].grounding_metadata\n",
        "  if grounding_metadata and grounding_metadata.search_entry_point:\n",
        "    display(HTML(grounding_metadata.search_entry_point.rendered_content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVlvJLcUTSuU"
      },
      "source": [
        "First try a query that needs realtime information, so you can see how the model performs _without_ Google Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qilzz3GKTdpl"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL)\n",
        "\n",
        "r = chat.send_message('Who won the most recent Australia vs Chinese Taipei games?')\n",
        "show_parts(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svT-7wNdTCCj"
      },
      "source": [
        "Now set up a new chat session that uses the `google_search` tool.  The `show_parts` helper will display the text output as well as any Google Search queries used in the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St4MxVo2Sl6I"
      },
      "outputs": [],
      "source": [
        "search_tool = {'google_search': {}}\n",
        "soccer_chat = client.chats.create(model=MODEL, config={'tools': [search_tool]})\n",
        "\n",
        "r = soccer_chat.send_message('Who won the most recent Australia vs Chinese Taipei games?')\n",
        "show_parts(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfkT7th0Tu1w"
      },
      "source": [
        "As you are using a `chat` session, you can ask the model follow-up questions too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L40g89mTyPs"
      },
      "outputs": [],
      "source": [
        "r = soccer_chat.send_message('Who scored the goals?')\n",
        "show_parts(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5liw_rs4T-gc"
      },
      "source": [
        "## Plot search results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VOKNOHLULMI"
      },
      "source": [
        "In this example you can see how to use the Google Search tool with code generation in order to plot results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n00CHfm0UKp1"
      },
      "outputs": [],
      "source": [
        "movie_chat = client.chats.create(model=MODEL, config={'tools': [search_tool]})\n",
        "\n",
        "r = movie_chat.send_message('Generate some Python code to plot the runtimes of the last 10 Denis Villeneuve movies.')\n",
        "show_parts(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JTk_5hnVw0r"
      },
      "source": [
        "First review the supplied code to make sure it does what you expect, then copy it here to try out the chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42UTFDweVwNm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Movie titles and runtimes (in minutes).\n",
        "# Data gathered from search results.\n",
        "movies = {\n",
        "    \"Dune: Part Two\": 166,\n",
        "    \"Dune\": 155,\n",
        "    \"Blade Runner 2049\": 164,\n",
        "    \"Arrival\": 116,\n",
        "    \"Sicario\": 121,\n",
        "    \"Enemy\": 91,\n",
        "    \"Prisoners\": 153,\n",
        "    \"Incendies\": 131,\n",
        "    \"Polytechnique\": 77,\n",
        "    \"Maelström\": 87\n",
        "}\n",
        "\n",
        "movie_titles = list(movies.keys())\n",
        "runtimes = list(movies.values())\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size for better readability\n",
        "plt.bar(movie_titles, runtimes, color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Movie Title\", fontsize=12)\n",
        "plt.ylabel(\"Runtime (minutes)\", fontsize=12)\n",
        "plt.title(\"Denis Villeneuve Movie Runtimes\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=10) # Rotate x-axis labels for better fit\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaMG2MmuWId3"
      },
      "source": [
        "One feature of using a chat conversation to do this is that you can now ask the model to make changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0vk3ol5WMvN"
      },
      "outputs": [],
      "source": [
        "r = movie_chat.send_message('Looks great! Can you give the chart a dark theme instead?')\n",
        "show_parts(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16B0usiYaTBH"
      },
      "source": [
        "Again, always be sure to review code generated by the model before running it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUjMCRzxaSCr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Movie titles and runtimes (in minutes).\n",
        "# Data gathered from search results.\n",
        "movies = {\n",
        "    \"Dune: Part Two\": 166,\n",
        "    \"Dune\": 155,\n",
        "    \"Blade Runner 2049\": 164,\n",
        "    \"Arrival\": 116,\n",
        "    \"Sicario\": 121,\n",
        "    \"Enemy\": 91,\n",
        "    \"Prisoners\": 153,\n",
        "    \"Incendies\": 131,\n",
        "    \"Polytechnique\": 77,\n",
        "    \"Maelström\": 87\n",
        "}\n",
        "\n",
        "movie_titles = list(movies.keys())\n",
        "runtimes = list(movies.values())\n",
        "\n",
        "# Set dark theme\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size for better readability\n",
        "plt.bar(movie_titles, runtimes, color='lightseagreen') #changed bar color\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Movie Title\", fontsize=12, color='white')\n",
        "plt.ylabel(\"Runtime (minutes)\", fontsize=12, color='white')\n",
        "plt.title(\"Denis Villeneuve Movie Runtimes\", fontsize=14, color='white')\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=10, color='white') # Rotate x-axis labels for better fit\n",
        "plt.yticks(color='white')\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUwwmbbRawwZ"
      },
      "source": [
        "## Use search in the Multimodal Live API\n",
        "\n",
        "The Search tool can be used in a live streaming context to have the model formulate grounded responses during the conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bax_JvhPdIry"
      },
      "source": [
        "### Define some helpers\n",
        "\n",
        "To use the bi-directional streaming API in Colab, you will buffer the audio stream. Define a `play_response` helper function to do the buffering, and once the audio for the current turn has completed, display an IPython audio widget.\n",
        "\n",
        "As each of the following examples only use a single prompt, also define a `run` helper to wrap the setup and prompt execution steps into a single function call. This helper takes a `config` argument that will be added to the `generation_config`, so that you can toggle the Search tool between examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEAdoUV2dH1o"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import io\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import wave\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "DEFAULT_OUTPUT_RATE = 24000\n",
        "BASE_MODEL_CONFIG = {\n",
        "    'generation_config' : {\n",
        "        # Here you can change the model's output mode between either audio or text.\n",
        "        # While this code expects an audio stream, text should work, but the stream\n",
        "        # may interleave with the `Buffering....` text.\n",
        "        'response_modalities': ['AUDIO']\n",
        "    },\n",
        "}\n",
        "\n",
        "async def play_response(stream):\n",
        "  \"\"\"Buffer audio output and display a widget. Returns the streamed responses.\"\"\"\n",
        "  turn_buf = io.BytesIO()\n",
        "  sample_rate = DEFAULT_OUTPUT_RATE\n",
        "\n",
        "  all_responses = []\n",
        "\n",
        "  print('Buffering', end='')\n",
        "  async for msg in stream.receive():\n",
        "    all_responses.append(msg)\n",
        "\n",
        "    if text:=msg.text:\n",
        "      print(text)\n",
        "    if audio_data := msg.data:\n",
        "      turn_buf.write(audio_data)\n",
        "      if m := re.search(\n",
        "          'rate=(?P<rate>\\d+)',\n",
        "          msg.server_content.model_turn.parts[0].inline_data.mime_type\n",
        "      ):\n",
        "            sample_rate = int(m.group('rate'))\n",
        "\n",
        "    elif tool_call := msg.tool_call:\n",
        "      # Handle tool-call requests. Here is where you would implement\n",
        "      # custom tool code, but for this example, all tools respond 'ok'.\n",
        "      for fc in tool_call.function_calls:\n",
        "        print('Tool call', end='')\n",
        "        tool_response = genai.types.LiveClientToolResponse(\n",
        "            function_responses=[genai.types.FunctionResponse(\n",
        "                name=fc.name,\n",
        "                id=fc.id,\n",
        "                response={'result': 'ok'},\n",
        "            )]\n",
        "        )\n",
        "        await stream.send(tool_response)\n",
        "\n",
        "    print('.', end='')\n",
        "\n",
        "  print()\n",
        "\n",
        "  # Play the audio\n",
        "  if turn_buf.tell():\n",
        "    audio = np.frombuffer(turn_buf.getvalue(), dtype=np.int16)\n",
        "    display(Audio(audio, autoplay=True, rate=sample_rate))\n",
        "  else:\n",
        "    print('No audio :(')\n",
        "    print(f'  {len(all_responses)=}')\n",
        "\n",
        "  return all_responses\n",
        "\n",
        "\n",
        "async def run(query, config=None):\n",
        "  # Add any tools or other generation config.\n",
        "  config = BASE_MODEL_CONFIG | (config or {})\n",
        "\n",
        "  # Establish a live session. While this context manager is active, the\n",
        "  # conversation will continue.\n",
        "  async with client.aio.live.connect(model=MODEL, config=config) as strm:\n",
        "\n",
        "    # Send the prompt.\n",
        "    await strm.send(query, end_of_turn=True)\n",
        "    # Handle the model response.\n",
        "    responses = await play_response(strm)\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMevh_9NodRJ"
      },
      "source": [
        "### Stream with the Search tool\n",
        "\n",
        "First, execute a query _without_ the Search tool to observe the model's response to a time-sensitive query.\n",
        "\n",
        "Note that the Multimodal Live API is a 2-way streaming API, but to simplify running in a notebook, each audio response is buffered and played once it has been fully streamed, so you will need to wait a few seconds before the response starts to play."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wXCugI3eOtS"
      },
      "outputs": [],
      "source": [
        "await run('Who won the skateboarding gold medals in the 2024 olympics?');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evvh0DgdpSBV"
      },
      "source": [
        "Now re-run with the Search tool enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYGEtdjupXM7"
      },
      "outputs": [],
      "source": [
        "responses = await run('Who won the skateboarding gold medals in the 2024 olympics?', {'tools': [search_tool]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_7R6nAMp9zf"
      },
      "source": [
        "If you wish to see the full output that was returned, you can enable `show_output` here and run this cell. It includes the complete audio binary data, so it is off by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UUZ5V_Upzpm"
      },
      "outputs": [],
      "source": [
        "show_output = False\n",
        "\n",
        "if show_output:\n",
        "  for msg in responses:\n",
        "    print(msg.model_dump(exclude_none=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtt8p612q5CT"
      },
      "source": [
        "### Search with custom tools\n",
        "\n",
        "In the Multimodal Live API, the Search tool can be used in conjunction with other tools, including function calls that you provide to the model.\n",
        "\n",
        "In this example, you define a function `set_climate` that takes 2 parameters, `mode` (`hot`, `cold`, etc) and `strength` (0-10), and ask the model to set the climate control based on the live weather in the location you specify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-nQ1Sp9yM33"
      },
      "outputs": [],
      "source": [
        "set_climate_tool = {'function_declarations': [{\n",
        "    'name': 'set_climate',\n",
        "    'description': 'Switches the local climate control equipment to the specified parameters.',\n",
        "    'parameters': {\n",
        "      'type': 'OBJECT',\n",
        "      'properties': {\n",
        "        # Define the \"mode\" argument.\n",
        "        'mode': {\n",
        "            'type': 'STRING',\n",
        "            'enum': [\n",
        "              # Define the possible values for \"mode\".\n",
        "              \"hot\",\n",
        "              \"cold\",\n",
        "              \"fan\",\n",
        "              \"off\",\n",
        "            ],\n",
        "            'description': 'Mode for the climate unit - whether to heat, cool or just blow air.',\n",
        "        },\n",
        "        # Define the \"strength\" argument.\n",
        "        'strength': {\n",
        "            'type': 'INTEGER',\n",
        "            'description': 'Intensity of the climate to apply, 0-10 (0 is off, 10 is MAX).',\n",
        "        },\n",
        "      },\n",
        "    },\n",
        "  },\n",
        "]}\n",
        "\n",
        "search_tool = {'google_search': {}}\n",
        "\n",
        "tools = {'tools': [search_tool, set_climate_tool]}\n",
        "\n",
        "responses = await run(\"Look up the weather in Paris and set my climate control appropriately.\", tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvZvdxr7oJ7i"
      },
      "source": [
        "Now inspect the `tool_call` response(s) you received during the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzZlE9IFnwpS"
      },
      "outputs": [],
      "source": [
        "for r in responses:\n",
        "  if tool := r.tool_call:\n",
        "    for fn in tool.function_calls:\n",
        "      args = ', '.join(f'{k}={v}' for k, v in fn.args.items())\n",
        "      print(f'{fn.name}({args})  # id={fn.id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOt32shZaEXj"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "<a name=\"next_steps\"></a>\n",
        "\n",
        "* For more demos showcasing multi-tool use in the Multimodal Live API, check out the [Plotting and Mapping cookbook](https://github.com/google-gemini/cookbook/blob/main/gemini-2/plotting_and_mapping.ipynb).\n",
        "* To learn more about tool use in the Live API, check out the [Live API Tool Use cookbook](https://github.com/google-gemini/cookbook/blob/main/gemini-2/live_api_tool_use.ipynb).\n",
        "* To get started with the Live API with the Python SDK, check out the [starter guide](https://github.com/google-gemini/cookbook/blob/main/gemini-2/live_api_starter.ipynb).\n",
        "\n",
        "Also check the other Gemini 2.0 capabilities (like [spatial understanding](../gemini-2/spatial_understanding.ipynb)) that you can find in the [Gemini Cookbook](https://github.com/google-gemini/cookbook/blob/main/gemini-2/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "search_tool.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}